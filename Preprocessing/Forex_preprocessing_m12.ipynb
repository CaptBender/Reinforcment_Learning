{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "#import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "SYMBOL_FACTOR = 100\n",
    "\n",
    "folder = \"data/GJ_short5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # Turn off warnings on creating new Year/Month/Time columns\n",
    "\n",
    "startTime = '2019-01-03 00:00:00'\n",
    "endTime = '2020-05-28 23:59:59'\n",
    "\n",
    "SYMBOL = 'GBPJPY'\n",
    "def prepareDF_signal(dataframe,getVolume, tf_string):\n",
    "    dataframe['Time']=pd.to_datetime(dataframe['Time'], format='%Y.%m.%d %H:%M')\n",
    "    #if not getVolume:\n",
    "    dataframe['Time'] = dataframe['Time'].shift(-1)\n",
    "    timeSeries_Start = pd.to_datetime(startTime)\n",
    "    timeSeries_End = pd.to_datetime(endTime)\n",
    "    dataframe1 = dataframe.loc[dataframe['Time'] >= timeSeries_Start]\n",
    "    dataframe3 = dataframe1.loc[dataframe1['Time'] <= timeSeries_End]\n",
    "    \n",
    "    if getVolume:\n",
    "        dataframe3['Year'] = pd.DatetimeIndex(dataframe3['Time']).year\n",
    "        dataframe3['Month'] = pd.DatetimeIndex(dataframe3['Time']).month\n",
    "        dataframe3['Day'] = pd.DatetimeIndex(dataframe3['Time']).day\n",
    "        dataframe3 = dataframe3[['Time', 'Year', 'Month', 'Day', 'Hour', 'Minute', 'Open', 'Close', 'High', 'Low', 'Volume', 'RSI', 'MA']]     \n",
    "        return dataframe3\n",
    "    \n",
    "    elif tf_string == '1440_':\n",
    "        dataframe4 = dataframe3.drop(['Hour','Minute','DayOfWeak', 'Open'], axis=1)\n",
    "        dataframe4.rename(columns={'High': tf_string+'High', 'Low': tf_string+'Low', 'Close': tf_string+'Close'}, inplace=True)\n",
    "        dataframe4 = dataframe4.iloc[:,:4]\n",
    "        return dataframe4\n",
    "    \n",
    "    else:\n",
    "        dataframe4 = dataframe3.drop(['Hour','Minute','DayOfWeak', 'Open', 'High', 'Low', 'Close'], axis=1)\n",
    "        dataframe4.rename(columns={'MA': tf_string+'MA', 'MA50': tf_string+'MA50','Volume': tf_string+'Volume', 'High': tf_string+'High', 'Low': tf_string+'Low', 'Close': tf_string+'Close',\n",
    "                                  'MACD_0': tf_string+'MACD_0', 'MACD_1': tf_string+'MACD_1', 'Momentum': tf_string+'Mom', 'RSI': tf_string+'RSI', 'ATR': tf_string+'ATR', 'StdDev': tf_string+'StdDev', 'Stochastic_0': tf_string+'stoc_0', 'Stochastic_1': tf_string+'stoc_1'}, inplace=True)\n",
    "    \n",
    "        return dataframe4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeDF(lowTF,highTF):\n",
    "    tf_merge = pd.merge(lowTF, highTF, on='Time', how='outer')\n",
    "    fill_merge = tf_merge.fillna(method='ffill')\n",
    "    return fill_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend(row):\n",
    "    if row.iloc[3] > row.iloc[4]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD CSV FILES WITH SIGNALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_csv = pd.read_csv(f'{folder}{SYMBOL}_1440.csv')\n",
    "day = prepareDF_signal(day_csv,False, '1440_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHour_csv = pd.read_csv(f'{folder}{SYMBOL}_60.csv')\n",
    "oneHour = prepareDF_signal(oneHour_csv,False, '60_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifteen_csv = pd.read_csv(f'{folder}{SYMBOL}_15.csv')\n",
    "fifteen = prepareDF_signal(fifteen_csv,False, '15_')\n",
    "fifteen['Trend'] = fifteen.apply(lambda row: trend(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_csv = pd.read_csv(f'{folder}{SYMBOL}_5.csv')\n",
    "five = prepareDF_signal(five_csv,True, '5_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERGE SIGNAL DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergefifteen = mergeDF(five,fifteen)\n",
    "mergeHour = mergeDF(mergefifteen,oneHour)\n",
    "mergeDay = mergeDF(mergeHour,day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot(df):\n",
    "    data = df.to_numpy()\n",
    "    h_l_c = data[:,-3:]\n",
    "    high = data[:,-3:-2]\n",
    "    low = data[:,-2:-1]\n",
    "    close = data[:,-1]\n",
    "    five_close = data[:,7:8]\n",
    "       \n",
    "    p = np.expand_dims(np.sum(h_l_c, axis=1)/3, axis=1)\n",
    "    r1 = np.subtract((2 * p), low)\n",
    "    r2 = np.subtract(np.add(p, high), low)\n",
    "    r3 = np.add(high, 2*(np.subtract(p, low)))\n",
    "    s1 = np.subtract((2 * p), high)\n",
    "    s2 = np.add(np.subtract(p, high), low)\n",
    "    s3 = np.subtract(low, 2*np.subtract(high, p))\n",
    "   \n",
    "    p_dist = np.subtract(five_close, p)*SYMBOL_FACTOR\n",
    "    r1_dist = np.subtract(five_close, r1)*SYMBOL_FACTOR\n",
    "    r2_dist = np.subtract(five_close, r2)*SYMBOL_FACTOR\n",
    "    r3_dist = np.subtract(five_close, r3)*SYMBOL_FACTOR\n",
    "    s1_dist = np.subtract(five_close, s1)*SYMBOL_FACTOR\n",
    "    s2_dist = np.subtract(five_close, s2)*SYMBOL_FACTOR\n",
    "    s3_dist = np.subtract(five_close, s3)*SYMBOL_FACTOR\n",
    "\n",
    "    return np.hstack((p_dist,r1_dist,r2_dist,r3_dist,s1_dist,s2_dist,s3_dist)) #, p, r1, r2, r3, s1, s2, s3\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = mergeDay.drop(['Time', 'Year', 'MA', '15_MA', '60_MA', '15_MA50', '60_MA50'], axis=1)\n",
    "new_df['Close_diff'] = (mergeDay['Close'].diff(periods=1))*SYMBOL_FACTOR\n",
    "new_df['high_low_diff'] = ((mergeDay.iloc[:,8:10].diff(axis=1, periods=-1)).iloc[:,:1])*SYMBOL_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['MA_trend'] = (mergeDay['MA'].diff(periods=1))*SYMBOL_FACTOR\n",
    "new_df['15_MA_trend'] = (mergeDay['15_MA'].diff(periods=1))*SYMBOL_FACTOR\n",
    "new_df['15_MA_dist'] = (mergeDay['Close'] - mergeDay['15_MA'])\n",
    "new_df['60_MA_dist'] = (mergeDay['Close'] - mergeDay['60_MA'])\n",
    "new_df['15_MA50_dist'] = (mergeDay['Close'] - mergeDay['15_MA50'])\n",
    "new_df['60_MA50_dist'] = (mergeDay['Close'] - mergeDay['60_MA50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = pd.DataFrame(pivot(mergeDay), columns=['p_dist','r1_dist','r2_dist','r3_dist','s1_dist','s2_dist','s3_dist']) #, 'p', 'r1', 'r2', 'r3', 's1', 's2', 's3'\n",
    "data_final = pd.concat([new_df, pivot_df], axis=1).iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_final.drop(['Open', '1440_High',\n",
    "       '1440_Low', '1440_Close'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USED FOR XGBOOST MODEL\n",
    "\n",
    "data_clean2 = data_clean[['Close', 'High', 'Low', 'Month', 'Day', 'Hour', 'Minute', 'Volume',\n",
    "       'RSI', '15_Volume', '15_ATR', '15_MACD_0', '15_MACD_1', '15_RSI',\n",
    "       '15_StdDev', '15_stoc_0', '15_stoc_1', '15_Buying Climax',\n",
    "       '15_Climactic Bar Strong', '15_Climactic Bar Weak',\n",
    "       '15_HV Effort To Fall', '15_HV Effort To Rise', '15_ND SO',\n",
    "       '15_NSP RSO', '15_Pseudo Reverse Upthrust', '15_Pseudo Upthrust',\n",
    "       '15_Reverse Shake Out', '15_Reverse Upthrust', '15_Selling Climax',\n",
    "       '15_Shake Out', '15_Stopping Volume', '15_Supply Test',\n",
    "       '15_Upthrust', '15_Smart Buying', '15_Smart Selling',\n",
    "       '60_Volume', '60_ATR', '60_MACD_0', '60_MACD_1', '60_RSI',\n",
    "       '60_StdDev', '60_stoc_0', '60_stoc_1', '60_Buying Climax',\n",
    "       '60_Climactic Bar Strong', '60_Climactic Bar Weak',\n",
    "       '60_HV Effort To Fall', '60_HV Effort To Rise', '60_ND SO',\n",
    "       '60_NSP RSO', '60_Pseudo Reverse Upthrust', '60_Pseudo Upthrust',\n",
    "       '60_Reverse Shake Out', '60_Reverse Upthrust', '60_Selling Climax',\n",
    "       '60_Shake Out', '60_Stopping Volume', '60_Supply Test',\n",
    "       '60_Upthrust', '60_Smart Buying', '60_Smart Selling', 'Close_diff',\n",
    "       'high_low_diff', 'MA_trend', '15_MA_trend', '15_MA_dist',\n",
    "       '60_MA_dist', '15_MA50_dist', '60_MA50_dist', 'Trend', 'p_dist', 'r1_dist',\n",
    "       'r2_dist', 'r3_dist', 's1_dist', 's2_dist', 's3_dist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USED FOR REINFORCEMENT LEARNING LSTM\n",
    "\n",
    "data_clean2 = data_clean[['Close', 'High', 'Low', 'Month', 'Day', 'Hour', 'Minute', 'Volume',\n",
    "       'RSI', '15_Volume', '15_ATR', '15_MACD_0', '15_MACD_1', '15_RSI',\n",
    "       '15_StdDev', '15_stoc_0', '15_stoc_1', 'Close_diff',\n",
    "       'high_low_diff', 'MA_trend', '15_MA_trend', '15_MA_dist',\n",
    "       '60_MA_dist', '15_MA50_dist', '60_MA50_dist', 'Trend', 'p_dist', 'r1_dist',\n",
    "       'r2_dist', 'r3_dist', 's1_dist', 's2_dist', 's3_dist',\n",
    "       '60_Volume', '60_ATR', '60_MACD_0', '60_MACD_1', '60_RSI',\n",
    "       '60_StdDev', '60_stoc_0', '60_stoc_1', '15_Buying Climax',\n",
    "       '15_Climactic Bar Strong', '15_Climactic Bar Weak',\n",
    "       '15_HV Effort To Fall', '15_HV Effort To Rise', '15_ND SO',\n",
    "       '15_NSP RSO', '15_Pseudo Reverse Upthrust', '15_Pseudo Upthrust',\n",
    "       '15_Reverse Shake Out', '15_Reverse Upthrust', '15_Selling Climax',\n",
    "       '15_Shake Out', '15_Stopping Volume', '15_Supply Test',\n",
    "       '15_Upthrust', '15_Smart Buying', '15_Smart Selling','60_Buying Climax',\n",
    "       '60_Climactic Bar Strong', '60_Climactic Bar Weak',\n",
    "       '60_HV Effort To Fall', '60_HV Effort To Rise', '60_ND SO',\n",
    "       '60_NSP RSO', '60_Pseudo Reverse Upthrust', '60_Pseudo Upthrust',\n",
    "       '60_Reverse Shake Out', '60_Reverse Upthrust', '60_Selling Climax',\n",
    "       '60_Shake Out', '60_Stopping Volume', '60_Supply Test',\n",
    "       '60_Upthrust', '60_Smart Buying', '60_Smart Selling']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STACKING SAMPLES FOR XGBOOST - EXPERIMENTAL\n",
    "from collections import deque\n",
    "SEQ_LEN = 6\n",
    "\n",
    "data_stack = np.empty([data_clean2.shape[0] - SEQ_LEN+1, SEQ_LEN*(data_clean2.shape[1]-10)])\n",
    "data_np = np.array(data_clean2.iloc[:,3:-7])\n",
    "chl = np.array(data_clean2.iloc[:,:3])\n",
    "pivot_num = np.array(data_clean2.iloc[:,-7:])\n",
    "data_deque = deque(maxlen=SEQ_LEN)\n",
    "\n",
    "i = 0\n",
    "for row in data_np:\n",
    "    data_deque.append(row)\n",
    "    if len(data_deque) == SEQ_LEN:\n",
    "        row_stack = np.stack(data_deque, axis=1).reshape(1,-1)\n",
    "        #row_stack = np.concatenate((chl[i:i+1], row_stack), axis=1)\n",
    "        #row_stack = np.concatenate((row_stack, pivot_num[i:i+1]), axis=1)\n",
    "        data_stack[i] = row_stack\n",
    "        i += 1\n",
    "stack = np.hstack((chl[SEQ_LEN-1:,:], data_stack, pivot_num[SEQ_LEN-1:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USED TO TEST FOR NAN VALUES IF ALL TIMEFRAMES AREN'T ALIGNED 100% BY START/END TIME\n",
    "testPD = data_clean2\n",
    "null_columns=testPD.columns[testPD.isnull().any()]\n",
    "\n",
    "testPD[null_columns].isnull().sum()\n",
    "print(testPD[testPD.isnull().any(axis=1)][null_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = data_clean2.to_csv(f'data\\\\{SYMBOL}_2015_2020_Smart_Diff_RL_V1b.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TARGET VALUE (Y) FOR DATASET FOR USE IN ORDINARY DNN OR XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATE Y AS A TAKE PROFIT VALUE. IF POSITIVE VALUE IT SHOULD BUY AND HOLD UNTIL VALUE. IF NEGATIVE VALUE IT SHOULD SELL AND HOLD UNTIL VALUE REACHED.\n",
    "\n",
    "y = [] \n",
    "counter = [0,0,0,0]\n",
    "index = 0\n",
    "for row in stack:\n",
    "    if index > stack.shape[0]-30:\n",
    "        print(index)\n",
    "        break\n",
    "    high = 0\n",
    "    low = 0\n",
    "    price = row[0]\n",
    "    for j in range(20):\n",
    "        if high < stack[index+j+1][1] - price: #High\n",
    "            high = stack[index+j+1][1] - price\n",
    "        if low > stack[index+j+1][2] - price: #Low\n",
    "            low = stack[index+j+1][2] - price\n",
    "    if high > abs(low):\n",
    "        y.append(high)\n",
    "        #y[i,1] = high\n",
    "        counter[0] += 1\n",
    "        counter[1] += high\n",
    "    else:\n",
    "        y.append(low)\n",
    "        #y[i,1] = low\n",
    "        counter[2] += 1\n",
    "        counter[3] += low\n",
    "    index += 1\n",
    "            \n",
    "print(f'DONE! High: {counter[0]}  Low: {counter[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATE Y AS CATEGORICAL VALUE: 0 - 1 - 2 (Should the model do nothing (0) - BUY (1) - SELL (2))\n",
    "y = np.empty((data_clean2.shape[0],2))\n",
    "counter = [0,0,0]\n",
    "for index, row in data_clean2.iterrows():\n",
    "    if index > data_clean2.shape[0]-30:\n",
    "        break\n",
    "    y[index,0] = index\n",
    "    price = row.iloc[0]\n",
    "    for i in range(18):\n",
    "        if data_clean2.iloc[index+i+1][1] > price + (20/SYMBOL_FACTOR): #HIGH\n",
    "            y[index,1] = 1\n",
    "            counter[1] += 1\n",
    "            break\n",
    "        elif data_clean2.iloc[index+i+1][2] < price-(20/SYMBOL_FACTOR): #LOW\n",
    "            y[index,1] = 2\n",
    "            counter[2] += 1\n",
    "            break\n",
    "        else:\n",
    "            if i == 11:\n",
    "                y[index,1] = 0\n",
    "                counter[0] += 1\n",
    "            \n",
    "print(f'DONE! Zero: {counter[0]}  Ones: {counter[1]}  Twos: {counter[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION\n",
    "#ADD Y VALUES TO DATASET\n",
    "data_adj = data_clean2.iloc[:-30,3:]\n",
    "data_adj['y'] = pd.DataFrame(y[:-30,1:])\n",
    "data_adj = data_adj.iloc[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_adj = np.concatenate((stack[:-29,3:], np.expand_dims(np.array(y), axis=1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF USING DATASET FOR XGBOOST WE NEED TO REMOVE OUTLIERS. A 4 STD. DIVIATION IS FOUND TO BE APPROPRIATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PANDAS\n",
    "#data_no_outliers = data_adj[np.abs(data_adj.y-data_adj.y.mean()) <= (4*data_adj.y.std())]\n",
    "\n",
    "#NUMPY\n",
    "data_no_outliers = data_adj[np.abs(data_adj[:,-1] - np.mean(data_adj[:,-1])) <= 4*np.std(data_adj[:,-1])]\n",
    "# keep only the ones that are within +/-4 standard deviations in the column 'y'.\n",
    "data_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_dt = data_no_outliers['y']\n",
    "y_count = np.fabs(np_dt)\n",
    "sum(y_count)/np_dt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing for NAN values\n",
    "testPD = data_adj\n",
    "null_columns=testPD.columns[testPD.isnull().any()]\n",
    "\n",
    "testPD[null_columns].isnull().sum()\n",
    "print(testPD[testPD.isnull().any(axis=1)][null_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BALANCE AND SHUFFLE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BALANCE VALUE SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = []\n",
    "short = []\n",
    "\n",
    "for index, row in data_adj.iterrows():\n",
    "   \n",
    "    if row['y'] >= 0:\n",
    "        long.append(row)\n",
    "    else:\n",
    "        short.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = min(len(long),len(short))\n",
    "\n",
    "long_df_s = (pd.DataFrame(long).sample(frac=1).reset_index(drop=True)).iloc[:min_df,:]\n",
    "short_df_s = (pd.DataFrame(short).sample(frac=1).reset_index(drop=True)).iloc[:min_df,:]\n",
    "\n",
    "df = long_df_s\n",
    "df = pd.concat([df, short_df_s])\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BALANCE CLASSIFICATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = []\n",
    "one_df = []\n",
    "two_df = []\n",
    "for index, row in data_adj.iterrows():\n",
    "    if int(row['y']) == 0:\n",
    "        null_df.append(row)\n",
    "    elif int(row['y']) == 1:\n",
    "        one_df.append(row)\n",
    "    elif int(row['y']) == 2:\n",
    "        two_df.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = min(len(null_df),len(one_df),len(two_df))\n",
    "\n",
    "null_df_s = (pd.DataFrame(null_df).sample(frac=1).reset_index(drop=True)).iloc[:min_df,:]\n",
    "one_df_s = (pd.DataFrame(one_df).sample(frac=1).reset_index(drop=True)).iloc[:min_df,:]\n",
    "two_df_s = (pd.DataFrame(two_df).sample(frac=1).reset_index(drop=True)).iloc[:min_df,:]\n",
    "df = null_df_s\n",
    "df = pd.concat([df, one_df_s])\n",
    "df = pd.concat([df, two_df_s])\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORT AS CSV OR NPY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = data_clean2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"data\\\\{SYMBOL}_2019_2020_Smart_Diff_RL_V2\", data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data\\\\WORK_Value_sample20_{SYMBOL}_2015_2019_Smart_Diff_Stack_V1.csv\", \"a\") as f:\n",
    "    np.savetxt(f, data_adj, fmt=\"%s\", newline=' ', delimiter=',')\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing for NAN values\n",
    "data_adj.isnull().any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = data_adj.to_csv(f'data\\\\WORK_Classif_sample18_{SYMBOL}_2015_2019_Smart_Diff_RLV1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras1",
   "language": "python",
   "name": "keras1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
