# Reinforcment Learning using DDDQN with Prioritized experience replay
*Master file: DDDQN_LSTM_XGB_PER.py

A pretty complex project so I will only describe some of the details.
This kind of project need a GPU otherwise it will take you years of training. I used a RTX 2080 ti which could run two sessions simultaniously.

In this project I created an autonomos model for forex trading. I wanted to experiment with reinforcement learning in another enviroment than just a game, as you usually see. Q-learning, or DQN, seems to be the best algorithm to start with when you want to learn reinforcement learning, so that's what I choose. There's quite a lot of caveats involved in making a DQN from scratch and it is even more important to unit test the separate parts of your code before you add them to your program. In DQN your targets are discrete values and in this case the model can choose from four different actions: 0 - Do nothing, 1 - buy the forexpair, 2 - sell the forexpair, 3 - close order. It can't go from buy directly to sell or opposite. It has to close (3) the previous order first. To prevent (teach) the model this, the program evaluted each action from the model and if needed adjusted the action to a legal action before it was executed. 

As forex is a timeseries problem, I primarily experimented using GRU and LSTM. The ladder gave the results even though I generally had a lot of problems with stability of the system due to the very noisy data you get from forex, or stocks for that matter. The number of samples in each stack didn't matter that much, so I mainly used a stack of 6 samples equivaliant to 30 minutes of data back in time. At first I used tick data(price data every second), but that gave me a dataset of app. 200Gigs which my computer wasn't happy about. :-D I ended up using samples with a frequency of 5 min. and five months of data.

For features I experimented alot and have probably tried about 50 combinations. Seasonality, such as month, weekday etc. doesn't matter much as I first thought. Instead I ended up using a lot of techniques from [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average), as well as known techniques from "Volume spread trading", which I have used myself in trading.

From DQN I improved the algorithm to also include double, dueling and Prioritized experience replay, making it a DDDQN with PER. At this point the code had gotten quite complex with a lot of Numpy calculations making the program slow to run. And when you use DQN you need to train it for a long time before it converge to minimum. Thus I had to improve the python code. For this I used Tracemalloc, to test for memory leaks, and to my surprise there was a problem with Sklearns MinMaxScaler, which made the code slower all the time due to a memoryleak. So I had to make a custom scaler, which was a bit slower than MinMaxScaler, but in the long run was way faster. Next I used blocks of "timeit" to time each codeblock and optimized my code a lot by rewriting bits of it.

I also experimented using a pretrained XGBoost algorith in combination with LSTM. XGBoost was used instead of the ordinay random actions a DQN makes in the beginning. In theory it should speed the learning process up by teaching the model how to do it correct and thereby converge faster. It works quite well and seems to be faster but I havn't made a comparable test yet. That's on my to-do list. :-)

I choose forex over stocks because forex markets are open from sunday evening to friday evening, which gives you more time to test when you test it live. Plus you avoid gaps in data which is very common in stocks because of pre- and postmarket. Gaps would probaly make it more difficult for the algorithm.
