{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext nb_black\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# from keras.layers.convolutional import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "#%load_ext tensorboard\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "#disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"data\\EURUSD_2019Day_HALF_UTCplus2_prepro.csv\"\n",
    "test_filename = \"data\\\\EURUSD_2019Day_HALF_UTCplus2_prepro.csv\"\n",
    "#filename = \"data\\\\testblok.csv\"\n",
    "\n",
    "\n",
    "# dataframe = pd.read_csv('data\\EURUSD_2017_HALF_UTCplus2_prepro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL = \"EURUSD\"\n",
    "SYMBOL_FACTOR = 10_000\n",
    "BOOK_NAME = \"orderbook\"\n",
    "LOAD_SCALER = True\n",
    "EPOCHS = 3\n",
    "REPLAY_MEMORY_SIZE = 1000_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 100_000\n",
    "MODEL_NAME = \"LSTM3_DQN_\"\n",
    "BATCHSIZE = 4096\n",
    "MINIBATCH_SIZE = 128\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "ENVIROMENT_OBSERVATION_SPACE = 139  # 138 from csv/signals + \"pip saldo\"\n",
    "ACTION_SPACE = (\n",
    "    4  # The four actions available for the Agent (Nothing, Buy, Sell, Close open order)\n",
    ")\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.90  # gamme\n",
    "EPISODES = 24\n",
    "EPSILON_DECAY = 0.95\n",
    "MIN_EPSILON = 0.01\n",
    "END_EPSILON_DECAYING = EPISODES // 2\n",
    "REPLAY = []\n",
    "LEARNING_PROGRESS = []\n",
    "MIN_WIN_LOSS = 10\n",
    "epsilon = 1\n",
    "\n",
    "difference = 0\n",
    "total_win_loss = 0\n",
    "prev_win_loss = 0\n",
    "prev_hour = 0\n",
    "orders = \"\"\n",
    "#log_dir = f\"logs\\\\{MODEL_NAME}_{int(time.time())}\"\n",
    "ep_rewards = []\n",
    "book_time = int(time.time())\n",
    "close_market = False\n",
    "\n",
    "\n",
    "# TENSORBOARD STUFF\n",
    "#tensorboard = tf.keras.callbacks.TensorBoard(log_dir = f\"logs\\\\{MODEL_NAME}_{int(time.time())}\")\n",
    "\n",
    "log_dir=\"logs\\\\\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"\\metrics\")\n",
    "#file_writer.set_as_default()\n",
    "\n",
    "#, update_freq= EPISODES, histogram_freq=1\n",
    "#lr_schedule_callback = LearningRateScheduler(schedule)\n",
    "\n",
    "\n",
    "\n",
    "# Create models folder\n",
    "if not os.path.isdir(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    global xdata, scaler\n",
    "    # Set initial points(pips) earned to 0\n",
    "    points_init = 0\n",
    "    points = np.array([points_init]).astype(\"float64\")\n",
    "    point_resp = np.reshape(points, (-1, 1))\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    np_df = data.to_numpy()\n",
    "    win_loss_proxy = np.random.uniform(\n",
    "        low=-1.0, high=1.0, size=data.shape[0]\n",
    "    ).reshape(-1, 1)\n",
    "    proxy_array = np.concatenate((np_df, win_loss_proxy), axis=1)\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scl = scaler.fit(proxy_array)\n",
    "\n",
    "\n",
    "    with open(f\"scaler/{SYMBOL}_scaler_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    xdata = proxy_array[:, :-1]\n",
    "    first_state = xdata[0:1, :]\n",
    "    current_state = np.concatenate((first_state, point_resp), axis=1)\n",
    "    return current_state.flatten(), xdata\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(current_state):\n",
    "    current_flat = current_state.flatten()\n",
    "    datetime_object = datetime(\n",
    "        year=int(current_flat[0]),\n",
    "        month=int(current_flat[1]),\n",
    "        day=int(current_flat[2]),\n",
    "        hour=int(current_flat[3]),\n",
    "        minute=int(current_flat[4]),\n",
    "    )\n",
    "    # print(datetime_object)\n",
    "    return datetime_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_check(current_state, action, orders, close_market):\n",
    "    # Check action if its legal and then send it to market or return a legal action if chosen action is illegal.\n",
    "    # 0 = do nothing   #1 = buy   #2 = sell   #3 = close order\n",
    "    if close_market:\n",
    "        if orders == \"long\":  # Is it end of day\n",
    "            action = 3\n",
    "        elif orders == \"short\":\n",
    "            action = 3\n",
    "        else:\n",
    "            action = 0\n",
    "    else:\n",
    "        if action == 0:\n",
    "            pass\n",
    "\n",
    "        elif action == 1:\n",
    "            if orders == \"long\":\n",
    "                action = 0\n",
    "            elif orders == \"short\":\n",
    "                action = 3\n",
    "\n",
    "        elif action == 2:\n",
    "            if orders == \"long\":\n",
    "                action = 3\n",
    "            elif orders == \"short\":\n",
    "                action = 0\n",
    "\n",
    "        elif action == 3:\n",
    "            if orders == \"long\":\n",
    "                action = 3\n",
    "            elif orders == \"short\":\n",
    "                action = 3\n",
    "            else:\n",
    "                action = 0\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_keeping(orders, orderprice, close_price, win_loss, current_state):\n",
    "    global total_win_loss\n",
    "    timestamp = to_datetime(current_state)\n",
    "    time_string = timestamp.strftime(\"%Y.%m.%d, %H:%M:%S\")\n",
    "    total_win_loss += win_loss\n",
    "    \n",
    "    # Create a numpy array with all the trade/state info for the current state\n",
    "    print(time_string + \", \" + orders + \", \" + str(win_loss) + \", \" + str(total_win_loss))\n",
    "    trade_info = (np.array([time_string, orders, orderprice, close_price, win_loss, total_win_loss]).reshape([1, -1]).flatten())\n",
    "    \n",
    "    state_sub = current_state.flatten()[5:138]\n",
    "    state_info = np.concatenate((trade_info, state_sub)).flatten()\n",
    "\n",
    "    with open(f\"output/{BOOK_NAME}_{book_time}.csv\", \"a\") as f:\n",
    "        np.savetxt(f, state_info, fmt=\"%s\", newline=\" \", delimiter=\",\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    return total_win_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_market(current_state, action, timestep, win_loss):\n",
    "    global orders, order_price, terminal_state, total_win_loss\n",
    "\n",
    "    current = current_state.flatten()\n",
    "    if action == 0:\n",
    "        if orders == 'long':\n",
    "            win_loss = current[5] - order_price\n",
    "        \n",
    "        elif orders == 'short':\n",
    "            win_loss = order_price - current[6]\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        close_order = False\n",
    "        #tot_w_l = 0\n",
    "    \n",
    "    if action == 1:\n",
    "        #Send buy order to market\n",
    "        orders = 'long'\n",
    "        order_price =  current[6] #Get 'ask' price from current state and saves it\n",
    "        close_order = False\n",
    "        #tot_w_l = 0\n",
    "    \n",
    "    elif action == 2:\n",
    "        #Send sell order to market\n",
    "        orders = 'short'\n",
    "        order_price = current[5] #Get 'bid' price from current state and saves it\n",
    "        close_order = False\n",
    "        #tot_w_l = 0\n",
    "    \n",
    "    elif action == 3 and orders =='long':\n",
    "        #Send close order to market\n",
    "        close_price = current[5]\n",
    "        win_loss = (close_price - order_price)*SYMBOL_FACTOR #close long order\n",
    "        total_win_loss = book_keeping(orders, order_price, close_price, win_loss, current_state) #Send trade info to CSV file\n",
    "        close_order = True\n",
    "        orders =''\n",
    "    \n",
    "    elif action == 3 and orders =='short':\n",
    "        #Send close order to market\n",
    "        close_price = current[6]\n",
    "        win_loss = (order_price - close_price)*SYMBOL_FACTOR #close short order\n",
    "        total_win_loss = book_keeping(orders, order_price, close_price, win_loss, current_state) #Send trade info to CSV file\n",
    "        close_order = True\n",
    "        orders =''\n",
    "    \n",
    "    #Update timestep and state\n",
    "    timestep += 1\n",
    "    if timestep == len(xdata-1):\n",
    "        terminal_state = True\n",
    "    else:\n",
    "        terminal_state = False\n",
    "    update_state = xdata[timestep]\n",
    "    update_state2 = update_state.flatten()\n",
    "    if win_loss > 100:\n",
    "        win_loss_adj = 100\n",
    "    elif win_loss < -100:\n",
    "        win_loss_adj = -100\n",
    "    else:\n",
    "        win_loss_adj = win_loss\n",
    "    wl = np.array(win_loss_adj/100).astype('float64')\n",
    "    wl_scaled = wl.flatten()\n",
    "    new_state = (np.concatenate((update_state2,wl_scaled))).reshape(-1,139)\n",
    "    \n",
    "    return new_state, terminal_state, win_loss, close_order, total_win_loss, timestep, orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(win_loss, prev_win_loss, close):\n",
    "    difference = win_loss - prev_win_loss\n",
    "    close_bonus = 0\n",
    "    \n",
    "    #BONUS FOR CLOSED ORDERS\n",
    "    if close:\n",
    "        if win_loss < 0:\n",
    "            close_bonus = -0.05\n",
    "        elif win_loss == 0:\n",
    "            close_bonus = 0\n",
    "        elif win_loss > 0:\n",
    "            close_bonus = 0.05\n",
    "    \n",
    "    #NEGATIVE\n",
    "    if win_loss < -10:  # Loss - minus\n",
    "        step_bonus = -0.005\n",
    "    elif win_loss > -10 and win_loss < 0:  # Loss - minus\n",
    "        step_bonus = -0.002\n",
    "    \n",
    "    #ZERO    \n",
    "    elif win_loss == 0:  # Loss - minus#Loss\n",
    "        step_bonus = 0\n",
    "        \n",
    "    #POSITIVE\n",
    "    elif win_loss > 0 and win_loss < 10:  # win\n",
    "        step_bonus = 0.002\n",
    "    elif win_loss > 10 and win_loss < 20:  # win\n",
    "        step_bonus = 0.005\n",
    "    elif win_loss > 20:  # win\n",
    "        step_bonus = 0.008\n",
    "    \n",
    "    reward = difference * step_bonus + close_bonus\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_day(current_state):\n",
    "    sample_time = to_datetime(current_state)\n",
    "    #if int(sample_time.hour) == 0 and int(sample_time.minute) > 5:\n",
    "    if int(sample_time.hour) == 23 and int(sample_time.minute) > 57:\n",
    "        day_over = True\n",
    "    else:\n",
    "        day_over = False\n",
    "    return day_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_done(current_state):\n",
    "    global prev_hour\n",
    "    current_flat = current_state.flatten()\n",
    "    if (current_flat[3] != prev_hour): \n",
    "        Episode_done = True\n",
    "    else:\n",
    "        Episode_done = False\n",
    "    prev_hour = current_flat[3]\n",
    "    return Episode_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #main model # gets trained every step\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        #Target model this is what we .predict against every step\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        \n",
    "        self.tensorboard = tensorboard #TensorBoard(log_dir=f\"logs/{MODEL_NAME}_{int(time.time())}\")\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(139,), activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "        #Model compile settings:\n",
    "        opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "    \n",
    "    def train(self, terminal_state):\n",
    "        global export\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        #print('0_',current_states.shape)\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        #print('1_',current_qs_list)\n",
    "    \n",
    "        new_states = np.array([transition[3] for transition in minibatch])\n",
    "        \n",
    "        future_qs_list = self.target_model.predict(new_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            qs = (current_qs_list[index]).flatten()\n",
    "            \n",
    "            #print('22_',qs, qs.shape, [index])\n",
    "            qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append((current_state).reshape(-1))\n",
    "            y.append(qs)\n",
    "            #print('X shape: ',np.array(X).shape,' y shape: ',np.array(y).shape)\n",
    "            \n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[tensorboard])\n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if done:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "            \n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(scaler.transform(state))\n",
    "    \n",
    "agent = DQNAgent()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    print(\"EPOCH #\", i, \" starting, of \", EPOCHS, \"epochs. Time: \")\n",
    "    if i == EPOCHS - 1:  # the last epoch, use test data set\n",
    "        EPISODES = 120\n",
    "        current_state, xdata = preprocess(test_filename)\n",
    "    else:\n",
    "        current_state, xdata = preprocess(train_filename)\n",
    "    win_loss = 0\n",
    "    step = 1\n",
    "    # Iterate over episodes\n",
    "    for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit=\"episodes\"):\n",
    "\n",
    "        # Update tensorboard step every episode\n",
    "        agent.tensorboard.step = episode\n",
    "\n",
    "        # Restarting episode - reset episode reward and step number\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Reset flag and start iterating until episode ends\n",
    "        done = False\n",
    "        \n",
    "        while close_market:\n",
    "            step +=1\n",
    "            minute = xdata[step:(step+1),4:5]\n",
    "            if 15 <= minute < 57:\n",
    "                close_market = False\n",
    "                done = False\n",
    "        \n",
    "        while not done:\n",
    "            done = episode_done(current_state)\n",
    "            if end_day(current_state):\n",
    "                close_market = True\n",
    "                done = True\n",
    "                \n",
    "\n",
    "            # This part stays mostly the same, the change is to query a model for Q values\n",
    "            if np.random.random() > epsilon:\n",
    "                # Get action from Q table\n",
    "                action = np.argmax(agent.get_qs(current_state))\n",
    "                print(\"Q-value action\",action)\n",
    "                action = action_check(current_state, action, orders, close_market)\n",
    "            else:\n",
    "                # Get random action\n",
    "                # print(\"Random action\")\n",
    "                action = np.random.randint(0, 4)\n",
    "                action = action_check(current_state, action, orders, close_market)\n",
    "\n",
    "            (\n",
    "                new_state,\n",
    "                terminal_state,\n",
    "                win_loss,\n",
    "                close,\n",
    "                total_win_loss,\n",
    "                step,\n",
    "                orders,\n",
    "            ) = to_market(current_state, action, step, win_loss)\n",
    "\n",
    "            reward = get_reward(win_loss, prev_win_loss, close)\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Every step we update replay memory and train main network\n",
    "            scaled_current = (scaler.transform(current_state.reshape(1, -1))).flatten()\n",
    "            \n",
    "            scaled_new_state = (scaler.transform(new_state.reshape(1, -1))).flatten()\n",
    "            agent.update_replay_memory((scaled_current, action, reward, scaled_new_state, done))\n",
    "            \n",
    "            agent.train(done)\n",
    "\n",
    "            # step += 1\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "            prev_win_loss = win_loss\n",
    "\n",
    "            market_close = False\n",
    "\n",
    "        # Append episode reward to a list and log stats (every given number of episodes)\n",
    "        ep_rewards.append(episode_reward)\n",
    "\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.scalar(\"Reward\", episode_reward, step=episode)\n",
    "            tf.summary.scalar(\"Pips\", win_loss, step=episode)\n",
    "\n",
    "        average_reward = sum(ep_rewards) / len(ep_rewards)\n",
    "        min_reward = min(ep_rewards)\n",
    "        max_reward = max(ep_rewards)\n",
    "        #agent.tensorboard.update_stats(\n",
    "        #    reward_avg=average_reward,\n",
    "        #    reward_min=min_reward,\n",
    "        #    reward_max=max_reward,\n",
    "        #    epsilon=epsilon,\n",
    "        #)\n",
    "\n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        if episode/50 == 0:  #total_win_loss >= MIN_WIN_LOSS:\n",
    "            agent.model.save(f\"models\\\\{MODEL_NAME}__{SYMBOL}__{int(time.time())}.model\")\n",
    "\n",
    "        # Decay epsilon\n",
    "        if epsilon > MIN_EPSILON:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "            epsilon = max(MIN_EPSILON, epsilon)\n",
    "    print(\"EPOCH #\", i, \" done, of \", epochs, \"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.model.save(f\"models\\\\{MODEL_NAME}__{SYMBOL}__{int(time.time())}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras1",
   "language": "python",
   "name": "keras1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
